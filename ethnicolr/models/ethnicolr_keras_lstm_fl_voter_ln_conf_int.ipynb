{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_last</th>\n",
       "      <th>name_first</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>841323</th>\n",
       "      <td>Torres</td>\n",
       "      <td>Jose</td>\n",
       "      <td>hispanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408926</th>\n",
       "      <td>Da Silva</td>\n",
       "      <td>Amanda</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733118</th>\n",
       "      <td>Mc Ghee</td>\n",
       "      <td>Sandra</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13104513</th>\n",
       "      <td>Karam</td>\n",
       "      <td>MELINDA</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9156114</th>\n",
       "      <td>Brewer</td>\n",
       "      <td>LAIA</td>\n",
       "      <td>nh_black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076722</th>\n",
       "      <td>Antunez Avila</td>\n",
       "      <td>Robert</td>\n",
       "      <td>hispanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10023679</th>\n",
       "      <td>Davis</td>\n",
       "      <td>WYATT</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5846252</th>\n",
       "      <td>Scott</td>\n",
       "      <td>Jacquelyn</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959131</th>\n",
       "      <td>Parton</td>\n",
       "      <td>Douglas</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12031764</th>\n",
       "      <td>Knapp</td>\n",
       "      <td>Liza</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              name_last name_first      race\n",
       "841323           Torres       Jose   hispanc\n",
       "1408926        Da Silva     Amanda  nh_white\n",
       "1733118         Mc Ghee     Sandra  nh_white\n",
       "13104513          Karam    MELINDA  nh_white\n",
       "9156114          Brewer       LAIA  nh_black\n",
       "...                 ...        ...       ...\n",
       "3076722   Antunez Avila     Robert   hispanc\n",
       "10023679          Davis      WYATT  nh_white\n",
       "5846252           Scott  Jacquelyn  nh_white\n",
       "5959131          Parton    Douglas  nh_white\n",
       "12031764          Knapp       Liza  nh_white\n",
       "\n",
       "[1000000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "NGRAMS = 2\n",
    "SAMPLE = 1000000\n",
    "EPOCHS = 15\n",
    "\n",
    "# Florida voter\n",
    "df = pd.read_csv('../dataverse_files/fl_reg_name_race.csv.gz')\n",
    "df.dropna(subset=['name_first', 'name_last'], inplace=True)\n",
    "sdf = df[df.race.isin(['multi_racial', 'native_indian', 'other', 'unknown']) == False].sample(SAMPLE, random_state=21)\n",
    "del df\n",
    "\n",
    "# Additional features\n",
    "sdf['name_last'] = sdf.name_last.str.title()\n",
    "\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_last</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>19431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hispanc</th>\n",
       "      <td>166865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_black</th>\n",
       "      <td>142675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_white</th>\n",
       "      <td>671029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name_last\n",
       "race               \n",
       "asian         19431\n",
       "hispanc      166865\n",
       "nh_black     142675\n",
       "nh_white     671029"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf = sdf.groupby('race').agg({'name_last': 'count'})\n",
    "rdf.to_csv('../dataverse_files/fl_voter_reg/lstm/fl_ln_race.csv', columns=[])\n",
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_last</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>9364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hispanc</th>\n",
       "      <td>41721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_black</th>\n",
       "      <td>23079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_white</th>\n",
       "      <td>145852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name_last\n",
       "race               \n",
       "asian          9364\n",
       "hispanc       41721\n",
       "nh_black      23079\n",
       "nh_white     145852"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.groupby('race').agg({'name_last': 'nunique'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_last</th>\n",
       "      <th>name_first</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>841323</th>\n",
       "      <td>Torres</td>\n",
       "      <td>Jose</td>\n",
       "      <td>hispanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408926</th>\n",
       "      <td>Da Silva</td>\n",
       "      <td>Amanda</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733118</th>\n",
       "      <td>Mc Ghee</td>\n",
       "      <td>Sandra</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13104513</th>\n",
       "      <td>Karam</td>\n",
       "      <td>MELINDA</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9156114</th>\n",
       "      <td>Brewer</td>\n",
       "      <td>LAIA</td>\n",
       "      <td>nh_black</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name_last name_first      race\n",
       "841323      Torres       Jose   hispanc\n",
       "1408926   Da Silva     Amanda  nh_white\n",
       "1733118    Mc Ghee     Sandra  nh_white\n",
       "13104513     Karam    MELINDA  nh_white\n",
       "9156114     Brewer       LAIA  nh_black"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words = 1166\n",
      "Max feature len = 26, Avg. feature len = 5\n"
     ]
    }
   ],
   "source": [
    "# last name only\n",
    "sdf['name_last_name_first'] = sdf['name_last']\n",
    "\n",
    "# build n-gram list\n",
    "vect = CountVectorizer(analyzer='char', max_df=0.3, min_df=3, ngram_range=(NGRAMS, NGRAMS), lowercase=False) \n",
    "a = vect.fit_transform(sdf.name_last_name_first)\n",
    "vocab = vect.vocabulary_\n",
    "\n",
    "# sort n-gram by freq (highest -> lowest)\n",
    "words = []\n",
    "for b in vocab:\n",
    "    c = vocab[b]\n",
    "    #print(b, c, a[:, c].sum())\n",
    "    words.append((a[:, c].sum(), b))\n",
    "    #break\n",
    "words = sorted(words, reverse=True)\n",
    "words_list = [w[1] for w in words]\n",
    "num_words = len(words_list)\n",
    "print(\"num_words = %d\" % num_words)\n",
    "\n",
    "\n",
    "def find_ngrams(text, n):\n",
    "    a = zip(*[text[i:] for i in range(n)])\n",
    "    wi = []\n",
    "    for i in a:\n",
    "        w = ''.join(i)\n",
    "        try:\n",
    "            idx = words_list.index(w)\n",
    "        except:\n",
    "            idx = 0\n",
    "        wi.append(idx)\n",
    "    return wi\n",
    "\n",
    "# build X from index of n-gram sequence\n",
    "X = np.array(sdf.name_last_name_first.apply(lambda c: find_ngrams(c, NGRAMS)))\n",
    "\n",
    "# check max/avg feature\n",
    "X_len = []\n",
    "for x in X:\n",
    "    X_len.append(len(x))\n",
    "\n",
    "max_feature_len = max(X_len)\n",
    "avg_feature_len = int(np.mean(X_len))\n",
    "\n",
    "print(\"Max feature len = %d, Avg. feature len = %d\" % (max_feature_len, avg_feature_len))\n",
    "y = np.array(sdf.race.astype('category').cat.codes)\n",
    "\n",
    "# Split train and test dataset\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a LSTM model\n",
    "\n",
    "ref: http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000 train sequences\n",
      "200000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (800000, 20)\n",
      "X_test shape: (200000, 20)\n",
      "4 classes\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (800000, 4)\n",
      "y_test shape: (200000, 4)\n"
     ]
    }
   ],
   "source": [
    "'''The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "Notes:\n",
    "\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "max_features = num_words # 20000\n",
    "feature_len = 20 # avg_feature_len # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=feature_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=feature_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build uncertainty model...\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 20, 32)            37312     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 120,260\n",
      "Trainable params: 120,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build uncertainty model...')\n",
    "\n",
    "#model_uncrtn = Sequential()\n",
    "input_ = keras.layers.Input(shape=(feature_len))\n",
    "layer_ = Embedding(num_words, 32) (input_)\n",
    "layer_ = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(layer_, training=True)\n",
    "#layer_ = Dropout(0.25)(layer_, training=True)\n",
    "output = Dense(num_classes, activation='softmax') (layer_)\n",
    "\n",
    "model_uncrtn = keras.models.Model(inputs=input_, outputs=output)\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_uncrtn.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model_uncrtn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/15\n",
      "22500/22500 [==============================] - 471s 21ms/step - loss: 0.6362 - accuracy: 0.7675 - val_loss: 0.6132 - val_accuracy: 0.7767\n",
      "Epoch 2/15\n",
      "22500/22500 [==============================] - 525s 23ms/step - loss: 0.5968 - accuracy: 0.7819 - val_loss: 0.5923 - val_accuracy: 0.7837\n",
      "Epoch 3/15\n",
      "22500/22500 [==============================] - 468s 21ms/step - loss: 0.5816 - accuracy: 0.7866 - val_loss: 0.5815 - val_accuracy: 0.7872\n",
      "Epoch 4/15\n",
      "22500/22500 [==============================] - 484s 22ms/step - loss: 0.5723 - accuracy: 0.7901 - val_loss: 0.5769 - val_accuracy: 0.7881\n",
      "Epoch 5/15\n",
      "22500/22500 [==============================] - 517s 23ms/step - loss: 0.5666 - accuracy: 0.7918 - val_loss: 0.5735 - val_accuracy: 0.7904\n",
      "Epoch 6/15\n",
      "22500/22500 [==============================] - 588s 26ms/step - loss: 0.5631 - accuracy: 0.7928 - val_loss: 0.5723 - val_accuracy: 0.7899\n",
      "Epoch 7/15\n",
      "22500/22500 [==============================] - 583s 26ms/step - loss: 0.5600 - accuracy: 0.7941 - val_loss: 0.5717 - val_accuracy: 0.7905\n",
      "Epoch 8/15\n",
      "22500/22500 [==============================] - 567s 25ms/step - loss: 0.5583 - accuracy: 0.7945 - val_loss: 0.5703 - val_accuracy: 0.7901\n",
      "Epoch 9/15\n",
      "22500/22500 [==============================] - 604s 27ms/step - loss: 0.5565 - accuracy: 0.7953 - val_loss: 0.5699 - val_accuracy: 0.7916\n",
      "Epoch 10/15\n",
      "22500/22500 [==============================] - 599s 27ms/step - loss: 0.5553 - accuracy: 0.7954 - val_loss: 0.5691 - val_accuracy: 0.7923\n",
      "Epoch 11/15\n",
      "22500/22500 [==============================] - 596s 26ms/step - loss: 0.5548 - accuracy: 0.7957 - val_loss: 0.5678 - val_accuracy: 0.7928\n",
      "Epoch 12/15\n",
      "22500/22500 [==============================] - 539s 24ms/step - loss: 0.5537 - accuracy: 0.7962 - val_loss: 0.5676 - val_accuracy: 0.7926\n",
      "Epoch 13/15\n",
      "22500/22500 [==============================] - 555s 25ms/step - loss: 0.5530 - accuracy: 0.7961 - val_loss: 0.5661 - val_accuracy: 0.7927\n",
      "Epoch 14/15\n",
      "22500/22500 [==============================] - 569s 25ms/step - loss: 0.5523 - accuracy: 0.7966 - val_loss: 0.5695 - val_accuracy: 0.7922\n",
      "Epoch 15/15\n",
      "22500/22500 [==============================] - 558s 25ms/step - loss: 0.5517 - accuracy: 0.7971 - val_loss: 0.5678 - val_accuracy: 0.7915\n",
      "6250/6250 [==============================] - 47s 7ms/step - loss: 0.5680 - accuracy: 0.7933\n",
      "Test score: 0.5680461525917053\n",
      "Test accuracy: 0.7933200001716614\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model_uncrtn.fit(X_train, y_train, batch_size=batch_size, epochs=EPOCHS,\n",
    "          validation_split=0.1, verbose=1)\n",
    "score, acc = model_uncrtn.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_uncrtn.save('../dataverse_files/fl_voter_reg/lstm/fl_all_ln_lstm_uncrtn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(words_list, columns=['vocab'])\n",
    "words_df.to_csv('../dataverse_files/fl_voter_reg/lstm/fl_all_ln_vocab.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==============================] - 50s 8ms/step\n",
      "6250/6250 [==============================] - 56s 9ms/step\n",
      "6250/6250 [==============================] - 56s 9ms/step\n",
      "6250/6250 [==============================] - 55s 9ms/step\n",
      "6250/6250 [==============================] - 56s 9ms/step\n",
      "6250/6250 [==============================] - 56s 9ms/step\n",
      "6250/6250 [==============================] - 56s 9ms/step\n",
      "6250/6250 [==============================] - 55s 9ms/step\n",
      "6250/6250 [==============================] - 55s 9ms/step\n",
      "6250/6250 [==============================] - 56s 9ms/step\n",
      "6250/6250 [==============================] - 46s 7ms/step\n",
      "6250/6250 [==============================] - 42s 7ms/step\n",
      "6250/6250 [==============================] - 45s 7ms/step\n",
      "6250/6250 [==============================] - 44s 7ms/step\n",
      "6250/6250 [==============================] - 47s 7ms/step\n",
      "6250/6250 [==============================] - 46s 7ms/step\n",
      "6250/6250 [==============================] - 44s 7ms/step\n",
      "6250/6250 [==============================] - 44s 7ms/step\n",
      "6250/6250 [==============================] - 52s 8ms/step\n",
      "6250/6250 [==============================] - 41s 7ms/step\n",
      "6250/6250 [==============================] - 40s 6ms/step\n",
      "6250/6250 [==============================] - 50s 8ms/step\n",
      "6250/6250 [==============================] - 54s 9ms/step\n",
      "6250/6250 [==============================] - 41s 7ms/step\n",
      "6250/6250 [==============================] - 38s 6ms/step\n",
      "6250/6250 [==============================] - 43s 7ms/step\n",
      "6250/6250 [==============================] - 46s 7ms/step\n",
      "6250/6250 [==============================] - 41s 7ms/step\n",
      "6250/6250 [==============================] - 44s 7ms/step\n",
      "6250/6250 [==============================] - 44s 7ms/step\n",
      "6250/6250 [==============================] - 40s 6ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 42s 7ms/step\n",
      "6250/6250 [==============================] - 43s 7ms/step\n",
      "6250/6250 [==============================] - 46s 7ms/step\n",
      "6250/6250 [==============================] - 45s 7ms/step\n",
      "6250/6250 [==============================] - 46s 7ms/step\n",
      "6250/6250 [==============================] - 38s 6ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 42s 7ms/step\n",
      "6250/6250 [==============================] - 40s 6ms/step\n",
      "6250/6250 [==============================] - 40s 6ms/step\n",
      "6250/6250 [==============================] - 49s 8ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 40s 6ms/step\n",
      "6250/6250 [==============================] - 48s 8ms/step\n",
      "6250/6250 [==============================] - 42s 7ms/step\n",
      "6250/6250 [==============================] - 42s 7ms/step\n",
      "6250/6250 [==============================] - 43s 7ms/step\n",
      "6250/6250 [==============================] - 41s 7ms/step\n",
      "6250/6250 [==============================] - 41s 7ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 35s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 40s 6ms/step\n",
      "6250/6250 [==============================] - 35s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 35s 6ms/step\n",
      "6250/6250 [==============================] - 37s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 37s 6ms/step\n",
      "6250/6250 [==============================] - 37s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 35s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 35s 6ms/step\n",
      "6250/6250 [==============================] - 37s 6ms/step\n",
      "6250/6250 [==============================] - 38s 6ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 36s 6ms/step\n",
      "6250/6250 [==============================] - 35s 6ms/step\n",
      "6250/6250 [==============================] - 35s 6ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 38s 6ms/step\n",
      "6250/6250 [==============================] - 41s 7ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 38s 6ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 40s 6ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 40s 6ms/step\n",
      "6250/6250 [==============================] - 40s 6ms/step\n",
      "6250/6250 [==============================] - 40s 6ms/step\n",
      "6250/6250 [==============================] - 40s 6ms/step\n",
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "6250/6250 [==============================] - 38s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "ITER=100\n",
    "\n",
    "for _ in range(ITER):\n",
    "    predictions.append(model_uncrtn.predict(X_test, verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_array = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a seperate array for each measurement\n",
    "mean_arr = predict_array.mean(axis=0).reshape(-1,4)\n",
    "std_arr = predict_array.std(axis=0).reshape(-1,4)\n",
    "pct_5_arr = np.quantile(predict_array, .05, axis=0).reshape(-1,4)\n",
    "pct_95_arr = np.quantile(predict_array, .95, axis=0).reshape(-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the class based on teh mean value with the highest probability\n",
    "final_pred_arr = mean_arr.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = list(sdf.race.astype('category').cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.DataFrame(columns = ['pred', 'category' , 'pred_5', 'pred_95', 'pred_se'])\n",
    "\n",
    "for i in range(len(final_pred_arr)):\n",
    "    pred = final_pred_arr[i]\n",
    "    category = target_names[pred]\n",
    "    pred_5 = pct_5_arr[i,pred]\n",
    "    pred_95 = pct_95_arr[i,pred]\n",
    "    pred_se = std_arr[i,pred]\n",
    "    \n",
    "    predict_df = predict_df.append({'pred':pred, \n",
    "                                    'category': category,\n",
    "                                    'pred_5':pred_5, \n",
    "                                    'pred_95': pred_95, \n",
    "                                    'pred_se':pred_se}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>category</th>\n",
       "      <th>pred_5</th>\n",
       "      <th>pred_95</th>\n",
       "      <th>pred_se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>nh_black</td>\n",
       "      <td>0.301045</td>\n",
       "      <td>0.825676</td>\n",
       "      <td>0.149403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>nh_black</td>\n",
       "      <td>0.393173</td>\n",
       "      <td>0.848633</td>\n",
       "      <td>0.137262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>nh_white</td>\n",
       "      <td>0.526817</td>\n",
       "      <td>0.810817</td>\n",
       "      <td>0.093785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>nh_white</td>\n",
       "      <td>0.772843</td>\n",
       "      <td>0.909658</td>\n",
       "      <td>0.047609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>nh_white</td>\n",
       "      <td>0.799318</td>\n",
       "      <td>0.910635</td>\n",
       "      <td>0.036062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>2</td>\n",
       "      <td>nh_black</td>\n",
       "      <td>0.457124</td>\n",
       "      <td>0.617795</td>\n",
       "      <td>0.045874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>3</td>\n",
       "      <td>nh_white</td>\n",
       "      <td>0.679935</td>\n",
       "      <td>0.835356</td>\n",
       "      <td>0.047263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>3</td>\n",
       "      <td>nh_white</td>\n",
       "      <td>0.820941</td>\n",
       "      <td>0.928417</td>\n",
       "      <td>0.035110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>3</td>\n",
       "      <td>nh_white</td>\n",
       "      <td>0.765600</td>\n",
       "      <td>0.909555</td>\n",
       "      <td>0.045077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>3</td>\n",
       "      <td>nh_white</td>\n",
       "      <td>0.424197</td>\n",
       "      <td>0.891610</td>\n",
       "      <td>0.136548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred  category    pred_5   pred_95   pred_se\n",
       "0         2  nh_black  0.301045  0.825676  0.149403\n",
       "1         2  nh_black  0.393173  0.848633  0.137262\n",
       "2         3  nh_white  0.526817  0.810817  0.093785\n",
       "3         3  nh_white  0.772843  0.909658  0.047609\n",
       "4         3  nh_white  0.799318  0.910635  0.036062\n",
       "...     ...       ...       ...       ...       ...\n",
       "199995    2  nh_black  0.457124  0.617795  0.045874\n",
       "199996    3  nh_white  0.679935  0.835356  0.047263\n",
       "199997    3  nh_white  0.820941  0.928417  0.035110\n",
       "199998    3  nh_white  0.765600  0.909555  0.045077\n",
       "199999    3  nh_white  0.424197  0.891610  0.136548\n",
       "\n",
       "[200000 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nh_white    158831\n",
       "hispanc      32441\n",
       "nh_black      6699\n",
       "asian         2029\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ci(name, num_iter=100, conf_int=0.9):\n",
    "    \n",
    "    NUM_CLASS = len(target_names)\n",
    "    low_quantile = 0.5 - (conf_int/2)\n",
    "    high_quantile = 0.5 + (conf_int/2)    \n",
    "    predictions = []\n",
    "    \n",
    "    np_name = np.array(find_ngrams(name, NGRAMS)).reshape(1,-1)\n",
    "    encoded_name = sequence.pad_sequences(np_name, maxlen=feature_len)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        predictions.append(model_uncrtn.predict(encoded_name))\n",
    "    \n",
    "    predict_array = np.array(predictions)\n",
    "    mean_arr = predict_array.mean(axis=0).reshape(-1,NUM_CLASS)\n",
    "    std_arr = predict_array.std(axis=0).reshape(-1,NUM_CLASS)\n",
    "    pct_low_arr = np.quantile(predict_array, low_quantile, axis=0).reshape(-1,NUM_CLASS)\n",
    "    pct_high_arr = np.quantile(predict_array, high_quantile, axis=0).reshape(-1,NUM_CLASS)\n",
    "    \n",
    "    class_pred = mean_arr.argmax(axis=1)\n",
    "    final_pred_class = target_names[class_pred[0]]\n",
    "    final_conf_val = mean_arr[0, class_pred[0]]\n",
    "    final_std_err = std_arr[0, class_pred[0]]\n",
    "    final_low_pct = pct_low_arr[0, class_pred[0]]\n",
    "    final_high_pct = pct_high_arr[0, class_pred[0]]\n",
    "    return final_pred_class, final_conf_val, final_std_err, [final_low_pct, final_high_pct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name, per_conf, std_err, conf_int = predict_ci(\"Wang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('asian', 0.9342278, 0.043742422, [0.8696241676807404, 0.9757247507572174])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name, per_conf, std_err, conf_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name, per_conf, std_err, conf_int = predict_ci(\"McMahon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nh_white', 0.82352144, 0.058932062, [0.7044001221656799, 0.9043974876403809])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name, per_conf, std_err, conf_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name, per_conf, std_err, conf_int = predict_ci(\"Sood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nh_white', 0.7761513, 0.11183323, [0.5674514651298522, 0.9091006010770798])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name, per_conf, std_err, conf_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_quant(name, num_iter=100, quant=[0.25,0.75]):\n",
    "    \n",
    "    NUM_CLASS = len(target_names)\n",
    "    NUM_QUANT = len(quant)\n",
    "    predictions = []\n",
    "    \n",
    "    np_name = np.array(find_ngrams(name, NGRAMS)).reshape(1,-1)\n",
    "    encoded_name = sequence.pad_sequences(np_name, maxlen=feature_len)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        predictions.append(model_uncrtn.predict(encoded_name))\n",
    "    \n",
    "    predict_array = np.array(predictions)\n",
    "    mean_arr = predict_array.mean(axis=0).reshape(-1,NUM_CLASS)\n",
    "    std_arr = predict_array.std(axis=0).reshape(-1,NUM_CLASS)\n",
    "    quant = sorted(quant)\n",
    "    quant_results = []\n",
    "    for i, val in enumerate(quant):\n",
    "        quant_results.append(np.quantile(predict_array, val, axis=0).reshape(-1,NUM_CLASS))\n",
    "\n",
    "    quant_results = np.array(quant_results).reshape(-1,NUM_CLASS)\n",
    "    class_pred = mean_arr.argmax(axis=1)\n",
    "    final_pred_class = target_names[class_pred[0]]\n",
    "    final_conf_val = mean_arr[0, class_pred[0]]\n",
    "    final_std_err = std_arr[0, class_pred[0]]\n",
    "    quant_final=[]\n",
    "    for i in range(NUM_QUANT):\n",
    "        quant_final.append(quant_results[i,class_pred[0]])\n",
    "    return final_pred_class, final_conf_val, final_std_err, quant_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name, per_conf, std_err, quant_res = predict_quant(\"Wang\", num_iter=10, quant=[.25,.5,.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('asian',\n",
       " 0.94205797,\n",
       " 0.015507944,\n",
       " [0.9330483078956604, 0.9386835396289825, 0.9581151753664017])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name, per_conf, std_err, quant_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name, per_conf, std_err, quant_res = predict_quant(\"Hernandez\", num_iter=10, quant=[.1,.4,.5,.5,.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hispanc',\n",
       " 0.832195,\n",
       " 0.014070588,\n",
       " [0.8170490086078643,\n",
       "  0.8255450367927551,\n",
       "  0.8331497311592102,\n",
       "  0.8331497311592102,\n",
       "  0.8471877157688141])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name, per_conf, std_err, quant_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name, per_conf, std_err, quant_res = predict_quant(\"Stewart\", num_iter=10, quant=[.05,.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nh_white', 0.7360791, 0.03686593, [0.684523293375969, 0.7893865704536438])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name, per_conf, std_err, quant_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
