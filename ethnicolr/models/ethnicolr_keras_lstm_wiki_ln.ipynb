{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 15:35:46.129367: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-21 15:35:46.129418: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "2.5.2\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_first</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Asian,GreaterEastAsian,EastAsian</th>\n",
       "      <td>5497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian,GreaterEastAsian,Japanese</th>\n",
       "      <td>7333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian,IndianSubContinent</th>\n",
       "      <td>7861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterAfrican,Africans</th>\n",
       "      <td>3672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterAfrican,Muslim</th>\n",
       "      <td>6242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,British</th>\n",
       "      <td>41445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,EastEuropean</th>\n",
       "      <td>8329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,Jewish</th>\n",
       "      <td>10239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,French</th>\n",
       "      <td>12293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Germanic</th>\n",
       "      <td>3869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Hispanic</th>\n",
       "      <td>10412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Italian</th>\n",
       "      <td>11867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Nordic</th>\n",
       "      <td>4813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       name_first\n",
       "race                                             \n",
       "Asian,GreaterEastAsian,EastAsian             5497\n",
       "Asian,GreaterEastAsian,Japanese              7333\n",
       "Asian,IndianSubContinent                     7861\n",
       "GreaterAfrican,Africans                      3672\n",
       "GreaterAfrican,Muslim                        6242\n",
       "GreaterEuropean,British                     41445\n",
       "GreaterEuropean,EastEuropean                 8329\n",
       "GreaterEuropean,Jewish                      10239\n",
       "GreaterEuropean,WestEuropean,French         12293\n",
       "GreaterEuropean,WestEuropean,Germanic        3869\n",
       "GreaterEuropean,WestEuropean,Hispanic       10412\n",
       "GreaterEuropean,WestEuropean,Italian        11867\n",
       "GreaterEuropean,WestEuropean,Nordic          4813"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "NGRAMS = 2\n",
    "EPOCHS = 25\n",
    "\n",
    "# Wikilabels\n",
    "df = pd.read_csv('../data/wiki/wiki_name_race.csv')\n",
    "df.dropna(subset=['name_first', 'name_last'], inplace=True)\n",
    "sdf = df\n",
    "\n",
    "# Additional features\n",
    "sdf['name_first'] = sdf.name_first.str.title()\n",
    "\n",
    "sdf.groupby('race').agg({'name_first': 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words = 1436\n",
      "Max feature len = 70, Avg. feature len = 5\n"
     ]
    }
   ],
   "source": [
    "# only last name will be use to train the model\n",
    "sdf['name_last_name_first'] = sdf['name_last'] \n",
    "\n",
    "# build n-gram list\n",
    "vect = CountVectorizer(analyzer='char', max_df=0.3, min_df=3, ngram_range=(NGRAMS, NGRAMS), lowercase=False) \n",
    "a = vect.fit_transform(sdf.name_last_name_first)\n",
    "vocab = vect.vocabulary_\n",
    "\n",
    "# sort n-gram by freq (highest -> lowest)\n",
    "words = []\n",
    "for b in vocab:\n",
    "    c = vocab[b]\n",
    "    #print(b, c, a[:, c].sum())\n",
    "    words.append((a[:, c].sum(), b))\n",
    "    #break\n",
    "words = sorted(words, reverse=True)\n",
    "words_list = ['UNK']\n",
    "words_list.extend([w[1] for w in words])\n",
    "num_words = len(words_list)\n",
    "print(\"num_words = %d\" % num_words)\n",
    "\n",
    "\n",
    "def find_ngrams(text, n):\n",
    "    a = zip(*[text[i:] for i in range(n)])\n",
    "    wi = []\n",
    "    for i in a:\n",
    "        w = ''.join(i)\n",
    "        try:\n",
    "            idx = words_list.index(w)\n",
    "        except:\n",
    "            idx = 0\n",
    "        wi.append(idx)\n",
    "    return wi\n",
    "\n",
    "# build X from index of n-gram sequence\n",
    "X = np.array(sdf.name_last_name_first.apply(lambda c: find_ngrams(c, NGRAMS)))\n",
    "\n",
    "# check max/avg feature\n",
    "X_len = []\n",
    "for x in X:\n",
    "    X_len.append(len(x))\n",
    "\n",
    "max_feature_len = max(X_len)\n",
    "avg_feature_len = int(np.mean(X_len))\n",
    "\n",
    "print(\"Max feature len = %d, Avg. feature len = %d\" % (max_feature_len, avg_feature_len))\n",
    "y = np.array(sdf.race.astype('category').cat.codes)\n",
    "\n",
    "# Split train and test dataset\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a LSTM model\n",
    "\n",
    "ref: http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107097 train sequences\n",
      "26775 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (107097, 20)\n",
      "X_test shape: (26775, 20)\n",
      "13 classes\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (107097, 13)\n",
      "y_test shape: (26775, 13)\n"
     ]
    }
   ],
   "source": [
    "'''The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "Notes:\n",
    "\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "\n",
    "max_features = num_words # 20000\n",
    "feature_len = 20 # avg_feature_len # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=feature_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=feature_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 32)            45952     \n",
      "_________________________________________________________________"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 15:35:58.783127: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-21 15:35:58.783169: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-21 15:35:58.783187: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-server-2): /proc/driver/nvidia/version does not exist\n",
      "2021-12-21 15:35:58.783373: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lstm (LSTM)                  (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 130,061\n",
      "Trainable params: 130,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 32, input_length=feature_len))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 15:35:59.169284: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-21 15:35:59.169847: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2250000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3013/3013 [==============================] - 115s 32ms/step - loss: 1.8290 - accuracy: 0.4477 - val_loss: 1.3863 - val_accuracy: 0.5914\n",
      "Epoch 2/25\n",
      "3013/3013 [==============================] - 104s 35ms/step - loss: 1.3748 - accuracy: 0.5946 - val_loss: 1.3145 - val_accuracy: 0.6168\n",
      "Epoch 3/25\n",
      "3013/3013 [==============================] - 105s 35ms/step - loss: 1.3074 - accuracy: 0.6142 - val_loss: 1.2721 - val_accuracy: 0.6337\n",
      "Epoch 4/25\n",
      "3013/3013 [==============================] - 102s 34ms/step - loss: 1.2602 - accuracy: 0.6278 - val_loss: 1.2434 - val_accuracy: 0.6402\n",
      "Epoch 5/25\n",
      "3013/3013 [==============================] - 104s 34ms/step - loss: 1.2234 - accuracy: 0.6405 - val_loss: 1.2274 - val_accuracy: 0.6486\n",
      "Epoch 6/25\n",
      "3013/3013 [==============================] - 104s 35ms/step - loss: 1.1895 - accuracy: 0.6514 - val_loss: 1.2111 - val_accuracy: 0.6534\n",
      "Epoch 7/25\n",
      "3013/3013 [==============================] - 102s 34ms/step - loss: 1.1771 - accuracy: 0.6538 - val_loss: 1.2048 - val_accuracy: 0.6548\n",
      "Epoch 8/25\n",
      "3013/3013 [==============================] - 104s 34ms/step - loss: 1.1535 - accuracy: 0.6608 - val_loss: 1.1964 - val_accuracy: 0.6581\n",
      "Epoch 9/25\n",
      "3013/3013 [==============================] - 105s 35ms/step - loss: 1.1280 - accuracy: 0.6694 - val_loss: 1.1923 - val_accuracy: 0.6612\n",
      "Epoch 10/25\n",
      "3013/3013 [==============================] - 104s 35ms/step - loss: 1.1191 - accuracy: 0.6715 - val_loss: 1.1878 - val_accuracy: 0.6600\n",
      "Epoch 11/25\n",
      "3013/3013 [==============================] - 104s 35ms/step - loss: 1.1037 - accuracy: 0.6756 - val_loss: 1.1903 - val_accuracy: 0.6649\n",
      "Epoch 12/25\n",
      "3013/3013 [==============================] - 105s 35ms/step - loss: 1.0933 - accuracy: 0.6786 - val_loss: 1.1863 - val_accuracy: 0.6630\n",
      "Epoch 13/25\n",
      "3013/3013 [==============================] - 102s 34ms/step - loss: 1.0710 - accuracy: 0.6853 - val_loss: 1.1836 - val_accuracy: 0.6659\n",
      "Epoch 14/25\n",
      "3013/3013 [==============================] - 104s 34ms/step - loss: 1.0741 - accuracy: 0.6854 - val_loss: 1.1847 - val_accuracy: 0.6687\n",
      "Epoch 15/25\n",
      "3013/3013 [==============================] - 118s 39ms/step - loss: 1.0606 - accuracy: 0.6874 - val_loss: 1.1850 - val_accuracy: 0.6697\n",
      "Epoch 16/25\n",
      "3013/3013 [==============================] - 127s 42ms/step - loss: 1.0609 - accuracy: 0.6869 - val_loss: 1.1812 - val_accuracy: 0.6728\n",
      "Epoch 17/25\n",
      "3013/3013 [==============================] - 127s 42ms/step - loss: 1.0484 - accuracy: 0.6912 - val_loss: 1.1831 - val_accuracy: 0.6718\n",
      "Epoch 18/25\n",
      "3013/3013 [==============================] - 126s 42ms/step - loss: 1.0396 - accuracy: 0.6936 - val_loss: 1.1835 - val_accuracy: 0.6717\n",
      "Epoch 19/25\n",
      "3013/3013 [==============================] - 127s 42ms/step - loss: 1.0411 - accuracy: 0.6928 - val_loss: 1.1827 - val_accuracy: 0.6723\n",
      "Epoch 20/25\n",
      "3013/3013 [==============================] - 125s 42ms/step - loss: 1.0404 - accuracy: 0.6933 - val_loss: 1.1828 - val_accuracy: 0.6718\n",
      "Epoch 21/25\n",
      "3013/3013 [==============================] - 124s 41ms/step - loss: 1.0235 - accuracy: 0.6973 - val_loss: 1.1838 - val_accuracy: 0.6707\n",
      "Epoch 22/25\n",
      "3013/3013 [==============================] - 123s 41ms/step - loss: 1.0271 - accuracy: 0.6948 - val_loss: 1.1825 - val_accuracy: 0.6738\n",
      "Epoch 23/25\n",
      "3013/3013 [==============================] - 122s 40ms/step - loss: 1.0193 - accuracy: 0.6974 - val_loss: 1.1873 - val_accuracy: 0.6742\n",
      "Epoch 24/25\n",
      "3013/3013 [==============================] - 123s 41ms/step - loss: 1.0155 - accuracy: 0.7004 - val_loss: 1.1813 - val_accuracy: 0.6717\n",
      "Epoch 25/25\n",
      "3013/3013 [==============================] - 123s 41ms/step - loss: 1.0115 - accuracy: 0.7004 - val_loss: 1.1903 - val_accuracy: 0.6716\n",
      "837/837 [==============================] - 6s 7ms/step - loss: 1.1823 - accuracy: 0.6721\n",
      "Test score: 1.1823115348815918\n",
      "Test accuracy: 0.6721194982528687\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=EPOCHS,\n",
    "          validation_split=0.1, verbose=1)\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837/837 - 8s\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "     Asian,GreaterEastAsian,EastAsian       0.82      0.76      0.79      1099\n",
      "      Asian,GreaterEastAsian,Japanese       0.83      0.87      0.85      1467\n",
      "             Asian,IndianSubContinent       0.70      0.67      0.69      1572\n",
      "              GreaterAfrican,Africans       0.52      0.37      0.43       734\n",
      "                GreaterAfrican,Muslim       0.57      0.55      0.56      1248\n",
      "              GreaterEuropean,British       0.72      0.87      0.78      8289\n",
      "         GreaterEuropean,EastEuropean       0.73      0.67      0.70      1666\n",
      "               GreaterEuropean,Jewish       0.45      0.34      0.39      2048\n",
      "  GreaterEuropean,WestEuropean,French       0.59      0.49      0.54      2459\n",
      "GreaterEuropean,WestEuropean,Germanic       0.39      0.28      0.33       774\n",
      "GreaterEuropean,WestEuropean,Hispanic       0.65      0.53      0.59      2082\n",
      " GreaterEuropean,WestEuropean,Italian       0.64      0.74      0.69      2374\n",
      "  GreaterEuropean,WestEuropean,Nordic       0.72      0.60      0.66       963\n",
      "\n",
      "                             accuracy                           0.67     26775\n",
      "                            macro avg       0.64      0.60      0.61     26775\n",
      "                         weighted avg       0.66      0.67      0.66     26775\n",
      "\n",
      "[[ 840   50   16    5   18  119    7    9   13    2    8   10    2]\n",
      " [  26 1278   14   13    5   62    9    7    3    1   31   16    2]\n",
      " [  13   18 1057   18  139  164   19   20   23    8   42   45    6]\n",
      " [  10   54   49  273   85  117    6   17   40    7   27   44    5]\n",
      " [  11   21  113   42  683  140   48   59   39   11   31   39   11]\n",
      " [  63   23   59   42   56 7189   63  223  268   49   60  128   66]\n",
      " [   8   17   30   14   29  132 1110  142   43   42   24   52   23]\n",
      " [  11   22   53   24   91  673  124  694   98   83   70   81   24]\n",
      " [  12    8   36   40   35  597   41  104 1208   44  127  188   19]\n",
      " [   3    2    6    4   10  229   17  145   47  218   14   39   40]\n",
      " [  12   25   32   18   20  258   23   61  145   38 1109  324   17]\n",
      " [   8   12   31   25   19  179   30   28  104   23  150 1757    8]\n",
      " [   6    3    8    8   15  188   17   42   25   33   12   26  580]]\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(X_test, verbose=2) # to predict probability\n",
    "y_pred = np.argmax(p, axis=-1)\n",
    "target_names = list(sdf.race.astype('category').cat.categories)\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./wiki/lstm/wiki_ln_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(words_list, columns=['vocab'])\n",
    "words_df.to_csv('./wiki/lstm/wiki_ln_vocab.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.groupby('race').agg({'name_first': 'count'}).to_csv('./wiki/lstm/wiki_race.csv', columns=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
